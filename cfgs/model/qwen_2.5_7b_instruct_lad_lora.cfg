[DISTRIBUTED]
deepspeed = True
bf16 = True
zero_stage = 2

[MODEL]
pretrained_model_dir = Qwen/Qwen2.5-7B-Instruct
tokenizer_dir = Qwen/Qwen2.5-7B-Instruct

[LAD]
lad = True
base_params_to_freeze = all
noise_schedule = identity
flow_representation_space = mapping
flow_representation_dim = 256
flow_representation_num_layers = 1
flow_representation_normalize = True
flow_representation_rescaling = mult
flow_to_lm_translation_depth = 2
flow_to_lm_hidden_size = 256
flow_to_lm_timestep_rescaling = 1.0
flow_to_lm_rescale_in_float32 = True
preserve_behavior_at_flow_start = True
modulate_hidden_states = False
full_dit_modulation = True
timestep_modulation_num_layers = 2
timestep_modulation_freq_embedding_size = 256
timestep_modulation_hidden_size = 256
freeze_modulation_at_flow_start = False
separate_flow_params = True
separate_flow_params_with_lora = True
flow_lora_rank = 32
flow_lora_alpha = 32.0
nstep_final_timestep = 1.0
nstep_x1_estimation = sample
nstep_normalize_x1_predictions = False
nstep_temperature_schedule = 0.0

# no analysis for faster training
compute_diff_at_training = False
compute_nstep_loss = -1

# signal to noise during training
minimum_training_noise = 0.0
minimum_training_noise_units = time
noise_rescaling = 64.0

[TRAINING]
micro_batch_size = 1
micro_valid_batch_size = 4
n_warmup_steps = 100
peak_lr = 0.0001
min_lr = 0.000001

[LOGGING/SAVING]
wandb_run_name = qwen2.5_7bi/lad_lora